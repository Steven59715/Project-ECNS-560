# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# Test more lambdas
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe)
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(
ames_cv,
grid = data.frame(penalty = lamdbdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# USE THE MODEL AS PREDICT ---------------
# Finalize the workflow
final_lasso = workflow_lasso |>
finalize_workflow(select_best(lasso_cv))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_lasso = final_lasso |> last_fit(ames_split)
fit_lasso |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_lasso |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# Make Predictions (use the predictions already created in the process)
# Use Augment()
test_prediction = fit_lasso |> augment()
# Plot predictions and the true values
ggplot(test_prediction, aes(x=.pred, y = Sale_Price)) +
geom_point(color = "blue") +
geom_abline()
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# Test more lambdas
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe)
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(
ames_cv,
grid = data.frame(penalty = lamdbdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# USE THE MODEL AS PREDICT ---------------
# Finalize the workflow
final_lasso = workflow_lasso |>
finalize_workflow(select_best(lasso_cv))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_lasso = final_lasso |> last_fit(ames_split)
fit_lasso |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_lasso |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# USE THE MODEL AS PREDICT ---------------
# Finalize the workflow
final_lasso = workflow_lasso |>
finalize_workflow(select_best(lasso_cv))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_lasso = final_lasso |> last_fit(ames_split)
fit_lasso |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_lasso |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# Make Predictions (use the predictions already created in the process)
# Use Augment()
test_prediction = fit_lasso |> augment()
# Plot predictions and the true values
ggplot(test_prediction, aes(x=.pred, y = Sale_Price)) +
geom_point(color = "blue") +
geom_abline()
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# Test more lambdas
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe)
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(
ames_cv,
grid = data.frame(penalty = lamdbdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
autoplot(lasso_cv, metric = "rmse")
# USE THE MODEL AS PREDICT ---------------
# Finalize the workflow
final_lasso = workflow_lasso |>
finalize_workflow(select_best(lasso_cv))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_lasso = final_lasso |> last_fit(ames_split)
fit_lasso |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_lasso |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# Define a recipe
# Pre-processing of the data, also called Feature Engineering
ames_recipeC = recipe(Sale_Price ~ ., data = ames_train) |>
step_dummy(all_nominal_predictors()) |>
step_nzv(all_predictors())
# Define a recipe
# Pre-processing of the data, also called Feature Engineering
ames_recipeC = recipe(Sale_Price ~ ., data = ames_train) |>
step_dummy(all_nominal_predictors()) |>
step_nzv(all_predictors())
# Process the data
ames_cleanC = ames_recipeC |> prep() |> juice()
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# define a model
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cv |> show_best()
# Test more lambdas
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe)
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(
ames_cv,
grid = data.frame(penalty = lamdbdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
# Process the data
ames_cleanC = ames_recipeC |> prep() |> juice()
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# define a model
# TUNE A LASSO REGRESSION ---------------------
model_ridge = linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
lasso_cvC = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cvC |> show_best()
# Process the data
ames_cleanC = ames_recipeC |> prep() |> juice()
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# define a model
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
lasso_cvC = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cvC |> show_best()
# Ridge Model
model_ridge = linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
lasso_cvC = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cvC |> show_best()
# Process the data
ames_cleanC = ames_recipeC |> prep() |> juice()
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# define a model
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cvC = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cvC |> show_best()
View(ames)
# Ridge Model
model_ridge = linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
ridge_cvC = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
ridge_cvC |> show_best()
final_ridge = workflow_ridge |>
finalize_workflow(select_best(ridge_cvC))
final_ridge = ridge_cvC |>
finalize_workflow(select_best(ridge_cvC))
autoplot(lasso_cv, metric = "rmse")
# USE THE RIDGE MODEL AS PREDICT ---------------
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_ridge = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipe)
# Finalize the workflow
final_ridge = workflow_ridge |>
finalize_workflow(select_best(ridge_cvC))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_ridge = final_ridge |> last_fit(ames_split)
fit_ridge |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_ridge |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# USE THE RIDGE MODEL AS PREDICT ---------------
lamdbdas = 10^seq(-5, 5, length = 1e3)
workflow_ridge = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipe)
# Finalize the workflow
final_ridge = workflow_ridge |>
finalize_workflow(select_best(ridge_cvC))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_ridge = final_ridge |> last_fit(ames_split)
fit_ridge |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_ridge |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# Make Predictions (use the predictions already created in the process)
# Use Augment()
test_predictionC = fit_ridgge |> augment()
# Make Predictions (use the predictions already created in the process)
# Use Augment()
test_predictionC = fit_ridge |> augment()
# Plot predictions and the true values
ggplot(test_predictionC, aes(x=.pred, y = Sale_Price)) +
geom_point(color = "blue") +
geom_abline()
View(test_predictionC)
View(test_predictionC)
# Look at coefficients from the best model
coefs = fit_ridge |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
Predicted_valuesC <- predict(fit_ridge, newdata = ames_test_B) #Predict based on fit
extract_workflow(fit_ridge)
y_predicted <- predict(model_ridge, s = best_lambda, newx = ames_test_B)
Predicted_valuesC <- predict(fit_ridge, newdata = ames_test_B) #Predict based on fit
# Model 1
predicted_values1 <- predict(ames_regress, newdata=ames_test) #Predict based on fit
sq_errors_pv1 = (predicted_values1 - ames_test$Sale_Price)^2
RMSE_PV1 = sqrt(mean(sq_errors_pv1))
# Model 2
Predicted_values2 <- predict(ames_regress2, newdata = ames_test) #Predict based on fit
sq_errors_pv2 = (Predicted_values2 - ames_test$Sale_Price)^2
RMSE_PV2 = sqrt(mean(sq_errors_pv2))
# Model 3
#Predicted_values3 <- predict(ames_regress3, newdata = ames_test) #Predict based on fit
# Errors from Roof_Matl variable, rerun the model without Roof_Matl as a predictor
ames_train_B <- subset( ames_train, select = -Roof_Matl ) # Remove Roof_Matl column from training data
ames_regress3B = lm(Sale_Price ~ ., data = ames_train_B)
Predicted_values3B <- predict(ames_regress3B, newdata = ames_test_B) #Predict based on fit
sq_errors_pv3B = (Predicted_values3B - ames_test$Sale_Price)^2
RMSE_PV3 = sqrt(mean(sq_errors_pv3B))
RMSE_PV1
RMSE_PV2
RMSE_PV3
# Model 1
predicted_values1 <- predict(ames_regress, newdata=ames_test) #Predict based on fit
sq_errors_pv1 = (predicted_values1 - ames_test$Sale_Price)^2
RMSE_PV1 = sqrt(mean(sq_errors_pv1))
# Model 2
Predicted_values2 <- predict(ames_regress2, newdata = ames_test) #Predict based on fit
sq_errors_pv2 = (Predicted_values2 - ames_test$Sale_Price)^2
RMSE_PV2 = sqrt(mean(sq_errors_pv2))
# Model 3
#Predicted_values3 <- predict(ames_regress3, newdata = ames_test) #Predict based on fit
# Errors from Roof_Matl variable, rerun the model without Roof_Matl as a predictor
ames_train_B <- subset( ames_train, select = -Roof_Matl ) # Remove Roof_Matl column from training data
ames_regress3B = lm(Sale_Price ~ ., data = ames_train_B)
Predicted_values3B <- predict(ames_regress3B, newdata = ames_test) #Predict based on fit
sq_errors_pv3B = (Predicted_values3B - ames_test$Sale_Price)^2
RMSE_PV3 = sqrt(mean(sq_errors_pv3B))
RMSE_PV1
RMSE_PV2
RMSE_PV3
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(readr)
library(readxl)
library(dplyr)
library(skimr)
library(binsreg)
library(splines)
library(sf)
library(estimatr)
library(fixest)
clean_data = read_csv("clean_data.csv")
clean_data = read_csv("/Cleaned Data/clean_data.csv")
clean_data = read_csv("Cleaned Data/clean_data.csv")
load("Cleaned Data/clean_data.RData")
summary(clean_data)
skim(clean_data)
# Create a histogram of the 'VALUE' column in the 'clean_data' data
ggplot(clean_data,aes(x=VALUE))+
geom_histogram(boundary=0,bindwith = 2)
# Create a histogram of the 'VALUE' column in the 'clean_data' data
ggplot(clean_data,aes(x=VALUE))+
geom_histogram(boundary=0,bindwith = 1000)
# Create a histogram of the 'VALUE' column in the 'clean_data' data
ggplot(clean_data,aes(x=VALUE))+
geom_histogram(boundary=0,binwidth = 1000)
values_over_10k = clean_data |> filter(VALUE>=10,000)
values_over_10k = clean_data |> filter(VALUE>=10000)
View(values_over_10k)
values_over_10k = clean_data |> filter(VALUE>=5000)
View(values_over_10k)
# Create a histogram of the 'NO_OF_INDIVIDUALS(INFLOW)' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=`NO_OF_INDIVIDUALS(INFLOW)`))+
geom_histogram(boundary=0,bindwith = 2)
# Create a histogram of the 'NO_OF_INDIVIDUALS(INFLOW)' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=`NO_OF_INDIVIDUALS(INFLOW)`))+
geom_histogram(boundary=0,bindwith = 2500)
# Create a histogram of the 'NO_OF_INDIVIDUALS(INFLOW)' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=`NO_OF_INDIVIDUALS(INFLOW)`))+
geom_histogram(boundary=0,binwidth = 2)
# Create a histogram of the 'NO_OF_INDIVIDUALS(INFLOW)' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=`NO_OF_INDIVIDUALS(INFLOW)`))+
geom_histogram(boundary=0,binwidth = 2500)
# Apply log transformation to VALUE and NO_OF_INDIVIDUALS(INFLOW) to create new log-transformed variables
clean_data$LOG_VALUE = log(clean_data$VALUE + 1)
clean_data$LOG_MIGRATION = log(clean_data$`NO_OF_INDIVIDUALS(INFLOW)` + 1)
# Create a histogram of the 'LOG_VALUE' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_VALUE))+
geom_histogram(boundary=0,bindwith = 0.5)
# Create a histogram of the 'LOG_VALUE' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_VALUE))+
geom_histogram(boundary=0,binwidth = 0.5)
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_MIGRATION))+
geom_histogram(boundary=0,bindwith = 0.5)
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_MIGRATION))+
geom_histogram(boundary=0,binwidth = 0.5)
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
migration_na = clean_data[is.na(clean_data$LOG_MIGRATION)]
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
migration_na = clean_data |> filter(is.na(LOG_MIGRATION))
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_MIGRATION))+
geom_histogram(boundary=0,binwidth = 0.5)
library(tidyverse)
library(stringr)
library(readr)
library(readxl)
library(dplyr)
library(skimr)
library(binsreg)
library(splines)
library(sf)
library(estimatr)
library(fixest)
# Load Dataframes
load("AvePRbyYear.RData")
setwd("C:/Users/Owner/Documents/GitHub/Project-ECNS-560")
load("AvePRbyYear.RData")
load("Cleaned Data/clean_data.RData")
# Create a working data frame so as to not destroy the original
data_temp = clean_data
data_temp = data_temp |>
mutate(AvePR = 0)
# Lookup the average prime interest rate for each year in the data set
for (i in seq(nrow(data_temp))){
index = data_temp$YEAR[i]
rate = AvePRbyYear$MeanPR[AvePRbyYear$Year==index]
data_temp$AvePR[i] = rate
}
save(data_temp, file = "data_temp.RData")
States = unique(data_temp$Y1_STATE)
Regions = c(rep('Pacific', 5), rep('Mountain', 8),
rep('West North Central', 7), rep('East North Central',5),
rep('West South Central', 4), rep('East South Central', 4),
rep('South Atlantic', 9), rep('Mid Atlantic',3), rep('New England',6))
RegNum = c(rep(1, 5), rep(2,8), rep(3,7), rep(4,4), rep(5,5), rep(6,4), rep(7,9), rep(8,3), rep(9,6))
State=c("WA", "OR", "CA", "AK",'HI',
"MT",'ID','WY','CO','UT','NM','AZ','NV',
'ND','SD','MN','IA','NE','KS','MO',
'WI','IL','IN','MI','OH',
'TX','OK','AR','LA',
'TN','KY','MS','AL',
'FL','GA','SC','NC','VA','WV','DC','DE','MD',
'NY','PA','NJ','RI','CT','MA','NH','VT','ME')
Region_States = data.frame(RegNum,Regions,State)
save(Region_States, file = "Region_States.RData")
for (i in seq(nrow(data_temp))){
index = data_temp$STATEPOSTAL[i]
region = Region_States$Regions[Region_States$State==index]
data_temp$Region[i] = region
}
data_temp = data_temp |>
mutate(Region = "US")
mutate(RegNum = 0)
data_temp = data_temp |>
mutate(Region = "US") |>
mutate(RegNum = 0)
for (i in seq(nrow(data_temp))){
index = data_temp$STATEPOSTAL[i]
region = Region_States$Regions[Region_States$State==index]
region_num = Region_States$RegNum[Region_States$State==index]
data_temp$RegNum[i] = region_num
data_temp$Region[i] = region
}
save(data_temp, file = "data_temp.RData")
View(clean_data)
data_temp = data_temp |>
mutate(AvePR = 0)
# Lookup the average prime interest rate for each year in the data set
for (i in seq(nrow(data_temp))){
index = data_temp$YEAR[i]
rate = AvePRbyYear$MeanPR[AvePRbyYear$Year==index]
data_temp$AvePR[i] = rate
}
data_temp = clean_data
# Create a column for the prime rate interest rate
data_temp = data_temp |>
mutate(AvePR = 0)
# Lookup the average prime interest rate for each year in the data set
for (i in seq(nrow(data_temp))){
index = data_temp$YEAR[i]
rate = AvePRbyYear$MeanPR[AvePRbyYear$Year==index]
data_temp$AvePR[i] = rate
}
View(data_temp)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results = 'hide')
# Create a scatter plot of LOG_MIGRATION vs. LOG_VALUE with a smoothing line
ggplot(clean_data, aes(x=LOG_MIGRATION, y =LOG_VALUE)) +
# Add points with low transparent
geom_point(alpha = 0.1)+
# Add a smooth line to show trend
geom_smooth()
clean_data_PR = clean_data |>
mutate(AvePR = 0)
for (i in seq(nrow(clean_data_PR))){
index = clean_data_PR$YEAR[i]
rate = AvePRbyYear$MeanPR[AvePRbyYear$Year==index]
clean_data_PR$AvePR[i] = rate
}
getwd
getwd()
#Saving the data
save(clean_data, file =  "Cleaned Data/clean_data.RData")
