ames_RMSE3
# Model 1
predicted_values1 <- predict(ames_regress, newdata=ames_test) #Predict based on fit
RMSE_PV1 = sqrt(mean(sq_errors_pv1))
sq_errors_pv1 = (predicted_values1 - ames_test$Sale_Price)^2
RMSE_PV1 = sqrt(mean(sq_errors_pv1))
RMSE_PV1
Predicted_values2 <- predict(ames_regress2, newdata = ames_test) #Predict based on fit
sq_errors_pv2 = (Predicted_values2 - ames_test$Sale_Price)
RMSE_PV2 = sqrt(mean(sq_errors_pv2))
RMSE_PV2
view(sq_errors_pv2)
sq_errors_pv2 = (Predicted_values2 - ames_test$Sale_Price)^2
RMSE_PV2 = sqrt(mean(sq_errors_pv2))
RMSE_PV2
Predicted_values3 <- predict(ames_regress3, newdata = ames_test) #Predict based on fit
ames_regress3B = lm(Sale_Price ~ .-Roof_Matl, data = ames_train)
View(ames_regress3B)
tidy(ames_regress3B)
Predicted_values3B <- predict(ames_regress3B, newdata = ames_test) #Predict based on fit
tidy(ames_regress3B)
tidy(ames_regress3)
ames_test_B <- subset( ames_test, select = -Roof_Matl )
ames_test_B <- subset( ames_test, select = -Roof_Matl )
Predicted_values3B <- predict(ames_regress3B, newdata = ames_test_B) #Predict based on fit
ames_train_B <- subset( ames_train, select = -Roof_Matl ) # Remove Roof_Matl column from training data
ames_regress3B = lm(Sale_Price ~ ., data = ames_train_B)
Predicted_values3B <- predict(ames_regress3B, newdata = ames_test_B) #Predict based on fit
sq_errors_pv3B = (Predicted_values3B - ames_test$Sale_Price)^2
RMSE_PV3 = sqrt(mean(sq_errors_pv3))
RMSE_PV3 = sqrt(mean(sq_errors_pv3B))
RMSE_PV3
RMSE_PV1
RMSE_PV2
RMSE_PV3
ames_recipe = recipe(Sale_Price ~ Year_Built + Gr_Liv_Area + Lot_Area + Bedroom_AbvGr+ Full_Bath + MS_SubClass + MS_Zoning + Neighborhood + Overall_Cond + Bldg_Type, data = ames_train) |>
step_dummy(all_nominal_predictors()) |>
step_nzv(all_predictors())
# Process the data
ames_clean = ames_recipe |> prep() |> juice()
# define a model
model_lm = linear_reg() |> set_engine("lm")
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# Define a work flow (put it all together)
ames_fit_lm_cv = workflow() |>
add_model(model_lm) |>
add_recipe(ames_recipe) |>
fit_resamples(ames_cv)
ames_fit_lm_cv |> collect_metrics(summarize = F)
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# Test more lambdas
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe)
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(
ames_cv,
grid = data.frame(penalty = lamdbdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# USE THE MODEL AS PREDICT ---------------
# Finalize the workflow
final_lasso = workflow_lasso |>
finalize_workflow(select_best(lasso_cv))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_lasso = final_lasso |> last_fit(ames_split)
fit_lasso |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_lasso |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# Make Predictions (use the predictions already created in the process)
# Use Augment()
test_prediction = fit_lasso |> augment()
# Plot predictions and the true values
ggplot(test_prediction, aes(x=.pred, y = Sale_Price)) +
geom_point(color = "blue") +
geom_abline()
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# Test more lambdas
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe)
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(
ames_cv,
grid = data.frame(penalty = lamdbdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# USE THE MODEL AS PREDICT ---------------
# Finalize the workflow
final_lasso = workflow_lasso |>
finalize_workflow(select_best(lasso_cv))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_lasso = final_lasso |> last_fit(ames_split)
fit_lasso |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_lasso |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# USE THE MODEL AS PREDICT ---------------
# Finalize the workflow
final_lasso = workflow_lasso |>
finalize_workflow(select_best(lasso_cv))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_lasso = final_lasso |> last_fit(ames_split)
fit_lasso |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_lasso |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# Make Predictions (use the predictions already created in the process)
# Use Augment()
test_prediction = fit_lasso |> augment()
# Plot predictions and the true values
ggplot(test_prediction, aes(x=.pred, y = Sale_Price)) +
geom_point(color = "blue") +
geom_abline()
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
# Test more lambdas
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe)
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(
ames_cv,
grid = data.frame(penalty = lamdbdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
autoplot(lasso_cv, metric = "rmse")
autoplot(lasso_cv, metric = "rmse")
# USE THE MODEL AS PREDICT ---------------
# Finalize the workflow
final_lasso = workflow_lasso |>
finalize_workflow(select_best(lasso_cv))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_lasso = final_lasso |> last_fit(ames_split)
fit_lasso |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_lasso |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# Define a recipe
# Pre-processing of the data, also called Feature Engineering
ames_recipeC = recipe(Sale_Price ~ ., data = ames_train) |>
step_dummy(all_nominal_predictors()) |>
step_nzv(all_predictors())
# Define a recipe
# Pre-processing of the data, also called Feature Engineering
ames_recipeC = recipe(Sale_Price ~ ., data = ames_train) |>
step_dummy(all_nominal_predictors()) |>
step_nzv(all_predictors())
# Process the data
ames_cleanC = ames_recipeC |> prep() |> juice()
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# define a model
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cv |> show_best()
# Test more lambdas
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_lasso = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe)
lasso_cv = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipe) |>
tune_grid(
ames_cv,
grid = data.frame(penalty = lamdbdas),
metrics = metric_set(rmse)
)
lasso_cv |> show_best()
# Process the data
ames_cleanC = ames_recipeC |> prep() |> juice()
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# define a model
# TUNE A LASSO REGRESSION ---------------------
model_ridge = linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
lasso_cvC = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cvC |> show_best()
# Process the data
ames_cleanC = ames_recipeC |> prep() |> juice()
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# define a model
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
lasso_cvC = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cvC |> show_best()
# Ridge Model
model_ridge = linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
lasso_cvC = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cvC |> show_best()
# Process the data
ames_cleanC = ames_recipeC |> prep() |> juice()
# set up cross-validation
ames_cv = ames_train |> vfold_cv(v=5)
# define a model
# TUNE A LASSO REGRESSION ---------------------
model_lasso = linear_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_cvC = workflow() |>
add_model(model_lasso) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
lasso_cvC |> show_best()
View(ames)
# Ridge Model
model_ridge = linear_reg(penalty = tune(), mixture = 0) |>
set_engine("glmnet")
ridge_cvC = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipeC) |>
tune_grid(ames_cv)
# Tune (which lamdas perform best)
ridge_cvC |> show_best()
final_ridge = workflow_ridge |>
finalize_workflow(select_best(ridge_cvC))
final_ridge = ridge_cvC |>
finalize_workflow(select_best(ridge_cvC))
autoplot(lasso_cv, metric = "rmse")
# USE THE RIDGE MODEL AS PREDICT ---------------
lamdbdas = 10^seq(-2, 5, length = 1e3)
workflow_ridge = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipe)
# Finalize the workflow
final_ridge = workflow_ridge |>
finalize_workflow(select_best(ridge_cvC))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_ridge = final_ridge |> last_fit(ames_split)
fit_ridge |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_ridge |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# USE THE RIDGE MODEL AS PREDICT ---------------
lamdbdas = 10^seq(-5, 5, length = 1e3)
workflow_ridge = workflow() |>
add_model(model_ridge) |>
add_recipe(ames_recipe)
# Finalize the workflow
final_ridge = workflow_ridge |>
finalize_workflow(select_best(ridge_cvC))
# fit model in full training data (all 5 folds)
# and make predictions in the test set
fit_ridge = final_ridge |> last_fit(ames_split)
fit_ridge |> collect_metrics()
# Look at coefficients from the best model
coefs = fit_ridge |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
# Make Predictions (use the predictions already created in the process)
# Use Augment()
test_predictionC = fit_ridgge |> augment()
# Make Predictions (use the predictions already created in the process)
# Use Augment()
test_predictionC = fit_ridge |> augment()
# Plot predictions and the true values
ggplot(test_predictionC, aes(x=.pred, y = Sale_Price)) +
geom_point(color = "blue") +
geom_abline()
View(test_predictionC)
View(test_predictionC)
# Look at coefficients from the best model
coefs = fit_ridge |>
extract_fit_parsnip() |>
tidy() |>
filter(estimate != 0) |>
view()
Predicted_valuesC <- predict(fit_ridge, newdata = ames_test_B) #Predict based on fit
extract_workflow(fit_ridge)
y_predicted <- predict(model_ridge, s = best_lambda, newx = ames_test_B)
Predicted_valuesC <- predict(fit_ridge, newdata = ames_test_B) #Predict based on fit
# Model 1
predicted_values1 <- predict(ames_regress, newdata=ames_test) #Predict based on fit
sq_errors_pv1 = (predicted_values1 - ames_test$Sale_Price)^2
RMSE_PV1 = sqrt(mean(sq_errors_pv1))
# Model 2
Predicted_values2 <- predict(ames_regress2, newdata = ames_test) #Predict based on fit
sq_errors_pv2 = (Predicted_values2 - ames_test$Sale_Price)^2
RMSE_PV2 = sqrt(mean(sq_errors_pv2))
# Model 3
#Predicted_values3 <- predict(ames_regress3, newdata = ames_test) #Predict based on fit
# Errors from Roof_Matl variable, rerun the model without Roof_Matl as a predictor
ames_train_B <- subset( ames_train, select = -Roof_Matl ) # Remove Roof_Matl column from training data
ames_regress3B = lm(Sale_Price ~ ., data = ames_train_B)
Predicted_values3B <- predict(ames_regress3B, newdata = ames_test_B) #Predict based on fit
sq_errors_pv3B = (Predicted_values3B - ames_test$Sale_Price)^2
RMSE_PV3 = sqrt(mean(sq_errors_pv3B))
RMSE_PV1
RMSE_PV2
RMSE_PV3
# Model 1
predicted_values1 <- predict(ames_regress, newdata=ames_test) #Predict based on fit
sq_errors_pv1 = (predicted_values1 - ames_test$Sale_Price)^2
RMSE_PV1 = sqrt(mean(sq_errors_pv1))
# Model 2
Predicted_values2 <- predict(ames_regress2, newdata = ames_test) #Predict based on fit
sq_errors_pv2 = (Predicted_values2 - ames_test$Sale_Price)^2
RMSE_PV2 = sqrt(mean(sq_errors_pv2))
# Model 3
#Predicted_values3 <- predict(ames_regress3, newdata = ames_test) #Predict based on fit
# Errors from Roof_Matl variable, rerun the model without Roof_Matl as a predictor
ames_train_B <- subset( ames_train, select = -Roof_Matl ) # Remove Roof_Matl column from training data
ames_regress3B = lm(Sale_Price ~ ., data = ames_train_B)
Predicted_values3B <- predict(ames_regress3B, newdata = ames_test) #Predict based on fit
sq_errors_pv3B = (Predicted_values3B - ames_test$Sale_Price)^2
RMSE_PV3 = sqrt(mean(sq_errors_pv3B))
RMSE_PV1
RMSE_PV2
RMSE_PV3
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(readr)
library(readxl)
library(dplyr)
library(skimr)
library(binsreg)
library(splines)
library(sf)
library(estimatr)
library(fixest)
clean_data = read_csv("clean_data.csv")
clean_data = read_csv("/Cleaned Data/clean_data.csv")
clean_data = read_csv("Cleaned Data/clean_data.csv")
load("Cleaned Data/clean_data.RData")
summary(clean_data)
skim(clean_data)
# Create a histogram of the 'VALUE' column in the 'clean_data' data
ggplot(clean_data,aes(x=VALUE))+
geom_histogram(boundary=0,bindwith = 2)
# Create a histogram of the 'VALUE' column in the 'clean_data' data
ggplot(clean_data,aes(x=VALUE))+
geom_histogram(boundary=0,bindwith = 1000)
# Create a histogram of the 'VALUE' column in the 'clean_data' data
ggplot(clean_data,aes(x=VALUE))+
geom_histogram(boundary=0,binwidth = 1000)
values_over_10k = clean_data |> filter(VALUE>=10,000)
values_over_10k = clean_data |> filter(VALUE>=10000)
View(values_over_10k)
values_over_10k = clean_data |> filter(VALUE>=5000)
View(values_over_10k)
# Create a histogram of the 'NO_OF_INDIVIDUALS(INFLOW)' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=`NO_OF_INDIVIDUALS(INFLOW)`))+
geom_histogram(boundary=0,bindwith = 2)
# Create a histogram of the 'NO_OF_INDIVIDUALS(INFLOW)' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=`NO_OF_INDIVIDUALS(INFLOW)`))+
geom_histogram(boundary=0,bindwith = 2500)
# Create a histogram of the 'NO_OF_INDIVIDUALS(INFLOW)' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=`NO_OF_INDIVIDUALS(INFLOW)`))+
geom_histogram(boundary=0,binwidth = 2)
# Create a histogram of the 'NO_OF_INDIVIDUALS(INFLOW)' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=`NO_OF_INDIVIDUALS(INFLOW)`))+
geom_histogram(boundary=0,binwidth = 2500)
# Apply log transformation to VALUE and NO_OF_INDIVIDUALS(INFLOW) to create new log-transformed variables
clean_data$LOG_VALUE = log(clean_data$VALUE + 1)
clean_data$LOG_MIGRATION = log(clean_data$`NO_OF_INDIVIDUALS(INFLOW)` + 1)
# Create a histogram of the 'LOG_VALUE' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_VALUE))+
geom_histogram(boundary=0,bindwith = 0.5)
# Create a histogram of the 'LOG_VALUE' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_VALUE))+
geom_histogram(boundary=0,binwidth = 0.5)
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_MIGRATION))+
geom_histogram(boundary=0,bindwith = 0.5)
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_MIGRATION))+
geom_histogram(boundary=0,binwidth = 0.5)
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
migration_na = clean_data[is.na(clean_data$LOG_MIGRATION)]
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
migration_na = clean_data |> filter(is.na(LOG_MIGRATION))
# Create a histogram of the 'LOG_MIGRATION' column in the 'clean_data' data frame
ggplot(clean_data,aes(x=LOG_MIGRATION))+
geom_histogram(boundary=0,binwidth = 0.5)
library(tidyverse)
library(stringr)
library(readr)
library(readxl)
library(dplyr)
library(skimr)
library(binsreg)
library(splines)
library(sf)
library(estimatr)
library(fixest)
# Load Dataframes
load("AvePRbyYear.RData")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stringr)
library(readr)
library(readxl)
library(dplyr)
library(skimr)
library(binsreg)
library(splines)
library(sf)
library(estimatr)
library(fixest)
# working directory needs to be set to the GitHub folder for the project
load("Cleaned Data/clean_data.RData")
View(clean_data)
View(clean_data)
setwd("C:/Users/Owner/Documents/GitHub/Project-ECNS-560")
data2=read_csv("countyinflow1920.csv")
#Immigration Data (Inflow Data)
#2019/2020
data2=read_csv("/Data/countyinflow1920.csv")
#Immigration Data (Inflow Data)
#2019/2020
data2=read_csv("Data/countyinflow1920.csv")
View(data2)
View(data2)
View(data2)
View(data2)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
#libraries needed
library(MASS)
library(tictoc)
library(future)
library(future.apply)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results = 'hide')
# working directory needs to be set to the GitHub folder for the project
load("Cleaned Data/clean_data.RData")
